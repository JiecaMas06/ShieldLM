seed: 0
run_mode: predict
output_dir: ./output

# Context（如需，也可在脚本中通过 set_context 设置，这里仅示意）
# context:
#   mode: 0            # GRAPH_MODE
#   device_target: Ascend
#   device_id: 0

# 模型结构采用内置 Qwen CausalLM，并依赖内置预设补齐默认超参
model:
  arch:
    type: QwenForCausalLM
  model_config:
    type: QwenConfig
  # 指向本地权重目录，触发从本地safetensors加载
  checkpoint_name_or_path: ./models/Qwen-14B-Chat

# 分词器：默认使用内置预设名称。完全离线时可改为本地目录
processor:
  tokenizer:
    type: QwenTokenizer
    pretrained_model_name_or_path: ./models/Qwen-14B-Chat
    padding_side: left

# 权重加载：指向本地 safetensors 目录，并声明格式
load_checkpoint: ./models/Qwen-14B-Chat
load_ckpt_format: safetensors
use_parallel: False
auto_trans_ckpt: False



# Inference config to load Qwen-14B-Chat via local safetensors (MindFormers 1.7.0)

seed: 0
run_mode: predict

context:
  mode: 0                 # GRAPH_MODE
  device_target: Ascend
  # device_id 可通过环境变量 DEVICE_ID 控制，通常无需在 YAML 固定

model:
  arch:
    type: QwenForCausalLM
  model_config:
    type: QwenConfig

# 覆盖权重为本地 safetensors（完整权重单卡加载）
load_checkpoint: ./models/Qwen-14B-Chat
load_ckpt_format: safetensors
use_parallel: False
auto_trans_ckpt: False

processor:
  tokenizer:
    type: QwenTokenizer
    # 完全离线：指向本地目录；若可联网也可用内置名 qwen_14b_chat
    pretrained_model_name_or_path: ./models/Qwen-14B-Chat
    padding_side: left



#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•Qwen3-14Bæ¨¡å‹çš„MindFormers generateå’Œinferæ¥å£
"""
import argparse
import importlib
import json
import os
from typing import Optional

import mindspore as ms
import numpy as np

from mindformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, MindFormerConfig
from mindformers.core.context import build_context
from mindformers.generation import GenerationConfig
from mindformers.tools.register import MindFormerRegister
from mindspore import Parameter


def parse_args():
    parser = argparse.ArgumentParser(description="æµ‹è¯•Qwen3æ¨¡å‹çš„generateå’Œinferæ¥å£")
    parser.add_argument("--config", dest="config_path", type=str, default=None,
                        help="MindFormers YAMLé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model_path", type=str, default=None,
                        help="æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆä¸ä½¿ç”¨YAMLæ—¶çš„å¤‡ç”¨é€‰é¡¹ï¼‰")
    parser.add_argument("--tokenizer_path", type=str, default=None,
                        help="åˆ†è¯å™¨ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨æ¨¡å‹è·¯å¾„")
    parser.add_argument("--test_mode", type=str, default="both",
                        choices=["generate", "infer", "both"],
                        help="æµ‹è¯•æ¨¡å¼ï¼šgenerate/infer/both")
    parser.add_argument("--prompt", type=str, default="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                        help="æµ‹è¯•ç”¨çš„æç¤ºè¯")
    parser.add_argument("--max_new_tokens", type=int, default=50,
                        help="ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡")
    parser.add_argument("--trust_remote_code", action="store_true",
                        help="æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ")
    parser.add_argument("--use_past", action="store_true", default=False,
                        help="æ˜¯å¦ä½¿ç”¨å¢é‡æ¨ç†ï¼ˆKV cacheï¼‰ï¼Œé»˜è®¤ä¸ä½¿ç”¨")
    parser.add_argument("--batch_size", type=int, default=1,
                        help="æ‰¹æ¬¡å¤§å°")
    return parser.parse_args()


def _import_model_register():
    """å¯¼å…¥Qwenç³»åˆ—æ¨¡å‹æ³¨å†Œæ¨¡å—"""
    try:
        print("æ­£åœ¨å¯¼å…¥Qwenæ¨¡å‹æ³¨å†Œæ¨¡å—...")
        importlib.import_module("mindformers.models.qwen2")
        print("âœ“ æˆåŠŸå¯¼å…¥ mindformers.models.qwen2")
        
        importlib.import_module("mindformers.models.qwen")
        print("âœ“ æˆåŠŸå¯¼å…¥ mindformers.models.qwen")
        
        try:
            importlib.import_module("mindformers.models.qwen3")
            print("âœ“ æˆåŠŸå¯¼å…¥ mindformers.models.qwen3")
        except Exception as e:
            print(f"âš  æœªæ‰¾åˆ°qwen3æ¨¡å—: {e}")
    except Exception as e:
        print(f"âš  æ¨¡å‹æ³¨å†Œå¯¼å…¥å¤±è´¥: {e}")


def _load_local_tokenizer(path_like: str, trust_remote_code: bool = False):
    """åŠ è½½æœ¬åœ°åˆ†è¯å™¨"""
    base = str(path_like)
    if os.path.isfile(base):
        base = os.path.dirname(base)
    if not os.path.isdir(base):
        raise RuntimeError(f"åˆ†è¯å™¨è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {base}")
    
    print(f"æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {base}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            base,
            padding_side="left",
            local_files_only=True,
            trust_remote_code=trust_remote_code,
        )
        print("âœ“ ä½¿ç”¨MindFormers AutoTokenizeråŠ è½½æˆåŠŸ")
        return tokenizer
    except Exception as e:
        print(f"âš  MindFormers AutoTokenizeråŠ è½½å¤±è´¥: {e}")
        print("å°è¯•ä½¿ç”¨HuggingFace AutoTokenizer...")
        from transformers import AutoTokenizer as HFAutoTokenizer
        tokenizer = HFAutoTokenizer.from_pretrained(
            base,
            padding_side="left",
            trust_remote_code=trust_remote_code,
            local_files_only=True,
        )
        print("âœ“ ä½¿ç”¨HuggingFace AutoTokenizeråŠ è½½æˆåŠŸ")
        return tokenizer


def check_checkpoint_files(model_path):
    """
    æ£€æŸ¥checkpointæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    """
    print("\n" + "="*60)
    print("æ£€æŸ¥Checkpointæ–‡ä»¶")
    print("="*60)
    
    if not model_path or not os.path.exists(model_path):
        print(f"âš ï¸ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    print(f"\næ¨¡å‹è·¯å¾„: {model_path}")
    
    # æ£€æŸ¥å„ç§å¯èƒ½çš„checkpointæ–‡ä»¶
    checkpoint_patterns = [
        "*.safetensors",
        "*.ckpt",
        "*.bin",
        "*.pth",
        "pytorch_model.bin",
        "model.safetensors",
    ]
    
    found_files = []
    total_size = 0
    
    for pattern in checkpoint_patterns:
        if '*' in pattern:
            # ä½¿ç”¨globæŸ¥æ‰¾
            import glob
            files = glob.glob(os.path.join(model_path, pattern))
            found_files.extend(files)
        else:
            # ç›´æ¥æ£€æŸ¥
            file_path = os.path.join(model_path, pattern)
            if os.path.exists(file_path):
                found_files.append(file_path)
    
    if found_files:
        print(f"\nâœ“ æ‰¾åˆ° {len(found_files)} ä¸ªcheckpointæ–‡ä»¶:")
        for f in found_files:
            size = os.path.getsize(f)
            total_size += size
            size_mb = size / (1024 * 1024)
            print(f"  - {os.path.basename(f)}: {size_mb:.2f} MB")
        
        total_size_gb = total_size / (1024 * 1024 * 1024)
        print(f"\næ€»å¤§å°: {total_size_gb:.2f} GB")
        
        # å¯¹äº14Bæ¨¡å‹ï¼ŒæœŸæœ›çš„å¤§å°å¤§çº¦æ˜¯ 28GB (bf16)
        expected_size_gb = 28
        if total_size_gb < expected_size_gb * 0.5:
            print(f"\nâš ï¸ è­¦å‘Šï¼šcheckpointæ–‡ä»¶å¤§å° ({total_size_gb:.2f} GB) è¿œå°äºé¢„æœŸ (~{expected_size_gb} GB)")
            print("  è¿™å¯èƒ½è¡¨æ˜checkpointæ–‡ä»¶ä¸å®Œæ•´")
        elif total_size_gb < expected_size_gb * 0.9:
            print(f"\nâš ï¸ checkpointæ–‡ä»¶å¤§å° ({total_size_gb:.2f} GB) ç•¥å°äºé¢„æœŸ (~{expected_size_gb} GB)")
        else:
            print(f"\nâœ“ checkpointæ–‡ä»¶å¤§å°æ­£å¸¸")
        
        return True
    else:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•checkpointæ–‡ä»¶ï¼")
        print("  æ¨¡å‹å¯èƒ½ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
        return False


def verify_model_weights(model, tokenizer=None):
    """
    éªŒè¯æ¨¡å‹æƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½
    
    æ£€æŸ¥å†…å®¹ï¼š
    1. æ˜¯å¦æœ‰å‚æ•°å…¨ä¸º0ï¼ˆè¡¨æ˜æœªåŠ è½½ï¼‰
    2. æ˜¯å¦æœ‰NaNæˆ–Infï¼ˆè¡¨æ˜åŠ è½½é”™è¯¯ï¼‰
    3. å…³é”®å±‚çš„ç»Ÿè®¡ä¿¡æ¯
    4. embeddingå±‚çš„æƒé‡æ£€æŸ¥
    """
    print("\n" + "="*60)
    print("éªŒè¯æ¨¡å‹æƒé‡åŠ è½½")
    print("="*60)
    
    all_params = []
    zero_params = []
    nan_inf_params = []
    suspicious_params = []
    
    # æ”¶é›†æ‰€æœ‰å‚æ•°
    print("\næ­£åœ¨æ£€æŸ¥æ¨¡å‹å‚æ•°...")
    for name, param in model.parameters_and_names():
        if isinstance(param, Parameter):
            try:
                # è½¬æ¢ä¸ºNumPyå¹¶ç¡®ä¿æ˜¯æ ‡å‡†ç±»å‹
                param_data = param.asnumpy()
                
                # è½¬æ¢ä¸ºfloat32ä»¥ç¡®ä¿å…¼å®¹æ€§
                if param_data.dtype not in [np.float32, np.float64, np.int32, np.int64]:
                    param_data = param_data.astype(np.float32)
                
                all_params.append((name, param_data))
                
                # æ£€æŸ¥å…¨é›¶å‚æ•°
                if np.all(param_data == 0):
                    zero_params.append(name)
                
                # æ£€æŸ¥NaNæˆ–Infï¼ˆä»…å¯¹æµ®ç‚¹ç±»å‹ï¼‰
                if param_data.dtype in [np.float32, np.float64]:
                    if np.any(np.isnan(param_data)) or np.any(np.isinf(param_data)):
                        nan_inf_params.append(name)
                
                # æ£€æŸ¥å¯ç–‘å‚æ•°ï¼ˆæ ‡å‡†å·®æå°ï¼‰
                if param_data.size > 1:
                    try:
                        # ç¡®ä¿æ˜¯æµ®ç‚¹ç±»å‹å†è®¡ç®—æ ‡å‡†å·®
                        if param_data.dtype not in [np.float32, np.float64]:
                            param_float = param_data.astype(np.float32)
                        else:
                            param_float = param_data
                        
                        std = np.std(param_float)
                        if std < 1e-10 and not np.all(param_data == 0):
                            suspicious_params.append((name, std))
                    except Exception as e:
                        # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªå‚æ•°
                        pass
            except Exception as e:
                print(f"  âš  æ— æ³•æ£€æŸ¥å‚æ•° {name}: {e}")
                continue
    
    print(f"âœ“ æ€»å…±æ£€æŸ¥äº† {len(all_params)} ä¸ªå‚æ•°")
    
    # æŠ¥å‘Šé—®é¢˜
    print("\n" + "="*60)
    print("æƒé‡æ£€æŸ¥ç»“æœ")
    print("="*60)
    
    if zero_params:
        print(f"\nâš ï¸ å‘ç° {len(zero_params)} ä¸ªå…¨é›¶å‚æ•°ï¼ˆå¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼‰:")
        for name in zero_params[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  - {name}")
        if len(zero_params) > 10:
            print(f"  ... è¿˜æœ‰ {len(zero_params) - 10} ä¸ª")
    else:
        print("\nâœ“ æ²¡æœ‰å‘ç°å…¨é›¶å‚æ•°")
    
    if nan_inf_params:
        print(f"\nâŒ å‘ç° {len(nan_inf_params)} ä¸ªåŒ…å«NaN/Infçš„å‚æ•°:")
        for name in nan_inf_params:
            print(f"  - {name}")
    else:
        print("âœ“ æ²¡æœ‰å‘ç°NaN/Infå‚æ•°")
    
    if suspicious_params:
        print(f"\nâš ï¸ å‘ç° {len(suspicious_params)} ä¸ªå¯ç–‘å‚æ•°ï¼ˆæ ‡å‡†å·®æå°ï¼‰:")
        for name, std in suspicious_params[:5]:
            print(f"  - {name}: std={std:.2e}")
        if len(suspicious_params) > 5:
            print(f"  ... è¿˜æœ‰ {len(suspicious_params) - 5} ä¸ª")
    
    # æ˜¾ç¤ºå…³é”®å±‚çš„ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*60)
    print("å…³é”®å±‚æƒé‡ç»Ÿè®¡")
    print("="*60)
    
    key_layers = ['embedding', 'wte', 'lm_head', 'output', 'attention', 'q_proj', 'k_proj', 'v_proj']
    
    displayed_count = 0
    for name, param_data in all_params:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å…³é”®å±‚
        is_key_layer = any(key in name.lower() for key in key_layers)
        
        if is_key_layer and displayed_count < 10:
            try:
                # ç¡®ä¿æ˜¯æµ®ç‚¹ç±»å‹
                if param_data.dtype not in [np.float32, np.float64]:
                    param_float = param_data.astype(np.float32)
                else:
                    param_float = param_data
                
                print(f"\nå‚æ•°: {name}")
                print(f"  - shape: {param_data.shape}")
                print(f"  - dtype: {param_data.dtype}")
                print(f"  - mean: {np.mean(param_float):.6f}")
                print(f"  - std: {np.std(param_float):.6f}")
                print(f"  - min: {np.min(param_float):.6f}")
                print(f"  - max: {np.max(param_float):.6f}")
                print(f"  - éé›¶å…ƒç´ æ¯”ä¾‹: {np.count_nonzero(param_data) / param_data.size * 100:.2f}%")
                displayed_count += 1
            except Exception as e:
                print(f"\nå‚æ•°: {name}")
                print(f"  - shape: {param_data.shape}")
                print(f"  - dtype: {param_data.dtype}")
                print(f"  âš  æ— æ³•è®¡ç®—ç»Ÿè®¡ä¿¡æ¯: {e}")
                displayed_count += 1
    
    # ç‰¹åˆ«æ£€æŸ¥embeddingå±‚
    print("\n" + "="*60)
    print("Embeddingå±‚ç‰¹åˆ«æ£€æŸ¥")
    print("="*60)
    
    embedding_found = False
    for name, param_data in all_params:
        if 'embedding' in name.lower() or 'wte' in name.lower():
            if not embedding_found:
                try:
                    print(f"\næ‰¾åˆ°embeddingå±‚: {name}")
                    print(f"  - shape: {param_data.shape}")
                    
                    # ç¡®ä¿æ˜¯æµ®ç‚¹ç±»å‹
                    if param_data.dtype not in [np.float32, np.float64]:
                        param_float = param_data.astype(np.float32)
                    else:
                        param_float = param_data
                    
                    # å¦‚æœæœ‰tokenizerï¼Œæµ‹è¯•å‡ ä¸ªå¸¸è§token
                    if tokenizer is not None:
                        print("\næµ‹è¯•å¸¸è§tokençš„embedding:")
                        test_tokens = {
                            "ä½ ": None,
                            "å¥½": None,
                            "hello": None,
                            "world": None,
                        }
                        
                        for text, _ in test_tokens.items():
                            try:
                                token_id = tokenizer.encode(text, add_special_tokens=False)
                                if isinstance(token_id, list) and len(token_id) > 0:
                                    token_id = token_id[0]
                                if token_id < param_float.shape[0]:
                                    embedding_vec = param_float[token_id]
                                    norm = np.linalg.norm(embedding_vec)
                                    mean_val = np.mean(embedding_vec)
                                    print(f"  - '{text}' (id={token_id}): norm={norm:.6f}, mean={mean_val:.6f}")
                            except Exception as e:
                                print(f"  - '{text}': æµ‹è¯•å¤±è´¥ ({e})")
                    
                    # éšæœºé‡‡æ ·å‡ ä¸ªembeddingæ£€æŸ¥
                    print("\néšæœºé‡‡æ ·5ä¸ªtokençš„embedding:")
                    sample_indices = np.random.choice(param_float.shape[0], size=min(5, param_float.shape[0]), replace=False)
                    for idx in sample_indices:
                        vec = param_float[idx]
                        norm = np.linalg.norm(vec)
                        is_zero = np.all(vec == 0)
                        print(f"  - Token {idx}: norm={norm:.6f}, is_zero={is_zero}")
                    
                    embedding_found = True
                    break
                except Exception as e:
                    print(f"\næ‰¾åˆ°embeddingå±‚: {name}")
                    print(f"  âš  æ— æ³•å®Œæˆembeddingæ£€æŸ¥: {e}")
                    embedding_found = True
                    break
    
    if not embedding_found:
        print("\nâš ï¸ æœªæ‰¾åˆ°embeddingå±‚")
    
    # æ€»ä½“è¯„ä¼°
    print("\n" + "="*60)
    print("æƒé‡åŠ è½½è¯„ä¼°")
    print("="*60)
    
    has_issues = len(zero_params) > 0 or len(nan_inf_params) > 0
    
    if has_issues:
        print("\nâŒ æƒé‡åŠ è½½å­˜åœ¨é—®é¢˜ï¼")
        if zero_params:
            print(f"  - {len(zero_params)} ä¸ªå‚æ•°ä¸ºå…¨é›¶")
        if nan_inf_params:
            print(f"  - {len(nan_inf_params)} ä¸ªå‚æ•°åŒ…å«NaN/Inf")
        print("\nå¯èƒ½çš„åŸå› ï¼š")
        print("  1. checkpointæ–‡ä»¶æŸåæˆ–ä¸å®Œæ•´")
        print("  2. æ¨¡å‹é…ç½®ä¸checkpointä¸åŒ¹é…")
        print("  3. åŠ è½½è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ä½†è¢«å¿½ç•¥")
        print("  4. safetensorsæ–‡ä»¶è¯»å–å¤±è´¥")
        print("\nå»ºè®®ï¼š")
        print("  - æ£€æŸ¥checkpointæ–‡ä»¶å®Œæ•´æ€§")
        print("  - æŸ¥çœ‹åŠ è½½æ—¥å¿—ä¸­çš„è­¦å‘Š/é”™è¯¯ä¿¡æ¯")
        print("  - éªŒè¯æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®")
    else:
        print("\nâœ“ æƒé‡åŠ è½½çœ‹èµ·æ¥æ­£å¸¸")
        print(f"  - æ‰€æœ‰ {len(all_params)} ä¸ªå‚æ•°éƒ½å·²åˆå§‹åŒ–")
        print("  - æ²¡æœ‰å‘ç°æ˜æ˜¾çš„åŠ è½½é—®é¢˜")
        
        if suspicious_params:
            print(f"\nâš ï¸ ä½†æœ‰ {len(suspicious_params)} ä¸ªå‚æ•°çš„æ ‡å‡†å·®æå°ï¼Œè¯·å…³æ³¨")
    
    print("="*60 + "\n")
    
    return not has_issues


def create_model_tokenizer(args):
    """åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨"""
    print("\n" + "="*60)
    print("å¼€å§‹åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨")
    print("="*60)
    
    cfg = None
    model = None
    tokenizer = None
    
    # æ–¹å¼1: ä½¿ç”¨YAMLé…ç½®æ–‡ä»¶
    if args.config_path and (args.config_path.endswith(".yaml") or args.config_path.endswith(".yml")):
        print(f"\nä½¿ç”¨YAMLé…ç½®æ–‡ä»¶: {args.config_path}")
        cfg = MindFormerConfig(args.config_path, run_mode="predict")
        
        if args.trust_remote_code:
            cfg.trust_remote_code = True
        
        # è®¾ç½®è®¾å¤‡ID
        device_id_str = os.getenv("DEVICE_ID", None)
        if device_id_str is not None:
            try:
                cfg.context.device_id = int(device_id_str)
                print(f"è®¾ç½®è®¾å¤‡ID: {cfg.context.device_id}")
            except Exception as e:
                print(f"âš  è®¾ç½®è®¾å¤‡IDå¤±è´¥: {e}")
        
        # æ„å»ºä¸Šä¸‹æ–‡
        build_context(cfg)
        print("âœ“ MindSporeä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ")
        
        # å¯¼å…¥æ¨¡å‹æ³¨å†Œ
        _import_model_register()
        
        # æ£€æŸ¥checkpointæ–‡ä»¶
        pretrained_dir = getattr(cfg, "pretrained_model_dir", None)
        if pretrained_dir is None and hasattr(cfg, "model") and hasattr(cfg.model, "model_config"):
            pretrained_dir = getattr(cfg.model.model_config, "pretrained_model_dir", None)
        if pretrained_dir:
            check_checkpoint_files(pretrained_dir)
        
        # åŠ è½½æ¨¡å‹ï¼ˆé‡è¦ï¼šä½¿ç”¨æ­£ç¡®çš„MindFormers APIåŠ è½½æƒé‡ï¼‰
        print("\næ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # è·å–YAMLæ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ï¼ˆåŒ…å«configçš„ç›®å½•ï¼‰
        yaml_dir = os.path.dirname(args.config_path) if args.config_path else None
        
        # å°è¯•ä½¿ç”¨AutoModel.from_pretrainedåŠ è½½ï¼ˆæŒ‡å‘YAMLæ‰€åœ¨ç›®å½•ï¼‰
        if yaml_dir and os.path.exists(yaml_dir):
            print(f"  å°è¯•ä½¿ç”¨AutoModel.from_pretrainedåŠ è½½")
            print(f"  YAMLç›®å½•: {yaml_dir}")
            try:
                # æ–¹å¼1ï¼šæŒ‡å‘åŒ…å«YAMLçš„ç›®å½•ï¼ŒMindFormersä¼šè‡ªåŠ¨è¯»å–YAMLå¹¶åŠ è½½æƒé‡
                model = AutoModel.from_pretrained(yaml_dir)
                print("âœ“ æ¨¡å‹åŠæƒé‡åŠ è½½æˆåŠŸï¼ˆAutoModel.from_pretrainedï¼‰")
            except Exception as e:
                print(f"âš  AutoModel.from_pretrainedå¤±è´¥: {e}")
                
                # æ–¹å¼2ï¼šå°è¯•ç›´æ¥æŒ‡å‘pretrained_model_dir
                if pretrained_dir and os.path.exists(pretrained_dir):
                    print(f"  å°è¯•ç›´æ¥åŠ è½½pretrained_dir: {pretrained_dir}")
                    try:
                        model = AutoModel.from_pretrained(pretrained_dir)
                        print("âœ“ æ¨¡å‹åŠæƒé‡åŠ è½½æˆåŠŸï¼ˆä»pretrained_dirï¼‰")
                    except Exception as e2:
                        print(f"âš  ä»pretrained_diråŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                        print("  å›é€€åˆ°from_config...")
                        model = AutoModel.from_config(args.config_path)
                        print("âš  ä½¿ç”¨from_configåˆ›å»ºæ¨¡å‹ç»“æ„ï¼ˆæƒé‡æœªåŠ è½½ï¼‰")
                else:
                    print("  å›é€€åˆ°from_config...")
                    model = AutoModel.from_config(args.config_path)
                    print("âš  ä½¿ç”¨from_configåˆ›å»ºæ¨¡å‹ç»“æ„ï¼ˆæƒé‡æœªåŠ è½½ï¼‰")
        else:
            print(f"âš  æœªæ‰¾åˆ°YAMLç›®å½•æˆ–pretrained_dir")
            model = AutoModel.from_config(args.config_path)
            print("âœ“ æ¨¡å‹ç»“æ„åˆ›å»ºæˆåŠŸï¼ˆæ— æƒé‡ï¼‰")
        
        # åŠ è½½åˆ†è¯å™¨
        print("\næ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
        tokenizer_cfg = getattr(getattr(cfg, "processor", None), "tokenizer", None)
        if tokenizer_cfg:
            try:
                tokenizer = MindFormerRegister.get_instance_from_cfg(tokenizer_cfg, "tokenizer")
                print("âœ“ ä»é…ç½®æ–‡ä»¶åŠ è½½åˆ†è¯å™¨æˆåŠŸ")
            except Exception as e:
                print(f"âš  ä»é…ç½®æ–‡ä»¶åŠ è½½åˆ†è¯å™¨å¤±è´¥: {e}")
        
        # å¤‡ç”¨åˆ†è¯å™¨åŠ è½½æ–¹å¼
        if tokenizer is None:
            tokenizer_path = args.tokenizer_path
            if tokenizer_path is None:
                pretrained_dir = getattr(cfg, "pretrained_model_dir", None)
                if pretrained_dir is None and hasattr(cfg, "model") and hasattr(cfg.model, "model_config"):
                    pretrained_dir = getattr(cfg.model.model_config, "pretrained_model_dir", None)
                tokenizer_path = pretrained_dir or args.model_path
            
            if tokenizer_path:
                tokenizer = _load_local_tokenizer(
                    tokenizer_path,
                    trust_remote_code=getattr(cfg, "trust_remote_code", args.trust_remote_code),
                )
    
    # æ–¹å¼2: ç›´æ¥ä½¿ç”¨æ¨¡å‹è·¯å¾„
    else:
        print(f"\nä½¿ç”¨æ¨¡å‹è·¯å¾„: {args.model_path}")
        device_id = int(os.getenv("DEVICE_ID", "0"))
        print(f"è®¾ç½®è®¾å¤‡ID: {device_id}")
        ms.set_context(mode=ms.GRAPH_MODE, device_target="Ascend", device_id=device_id)
        
        _import_model_register()
        
        # æ£€æŸ¥checkpointæ–‡ä»¶
        if args.model_path:
            check_checkpoint_files(args.model_path)
        
        print("\næ­£åœ¨ä»è·¯å¾„åŠ è½½æ¨¡å‹...")
        try:
            # ä½¿ç”¨AutoModel.from_pretrainedï¼ˆMindFormersçš„æ­£ç¡®APIï¼‰
            model = AutoModel.from_pretrained(args.model_path)
            print("âœ“ æ¨¡å‹åŠæƒé‡åŠ è½½æˆåŠŸï¼ˆAutoModel.from_pretrainedï¼‰")
        except Exception as e:
            print(f"âš  AutoModel.from_pretrainedå¤±è´¥: {e}")
            print("  è¯·ç¡®ä¿æ¨¡å‹è·¯å¾„åŒ…å«YAMLé…ç½®æ–‡ä»¶æˆ–æ˜¯æ”¯æŒçš„æ¨¡å‹åç§°")
            raise
        
        tokenizer_path = args.tokenizer_path or args.model_path
        tokenizer = _load_local_tokenizer(tokenizer_path, trust_remote_code=args.trust_remote_code)
    
    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.set_train(False)
    print("âœ“ æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
    
    # ç¡®ä¿åˆ†è¯å™¨æœ‰å¿…è¦çš„ç‰¹æ®Štoken
    if getattr(tokenizer, "eos_token", None) is None:
        tokenizer.eos_token = "<|endoftext|>"
        print(f"è®¾ç½®eos_token: {tokenizer.eos_token}")
    if getattr(tokenizer, "pad_token", None) is None:
        tokenizer.pad_token = tokenizer.eos_token
        print(f"è®¾ç½®pad_token: {tokenizer.pad_token}")
    
    print("\n" + "="*60)
    print("æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å®Œæˆ")
    print("="*60 + "\n")
    
    # éªŒè¯æƒé‡åŠ è½½
    weights_ok = verify_model_weights(model, tokenizer)
    if not weights_ok:
        print("\nâŒ è­¦å‘Šï¼šæƒé‡åŠ è½½éªŒè¯æœªé€šè¿‡ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œï¼")
        print("å»ºè®®ï¼šæ£€æŸ¥ä¸Šé¢çš„è¯¦ç»†ä¿¡æ¯ï¼Œç¡®è®¤checkpointæ–‡ä»¶æ˜¯å¦æ­£ç¡®\n")
    
    return model, tokenizer


def quick_forward_test(model, tokenizer):
    """
    å¿«é€Ÿå‰å‘ä¼ æ’­æµ‹è¯•ï¼ŒéªŒè¯æ¨¡å‹èƒ½å¦æ­£å¸¸è¾“å‡º
    """
    print("\n" + "="*60)
    print("å¿«é€Ÿå‰å‘ä¼ æ’­æµ‹è¯•")
    print("="*60)
    
    try:
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è¾“å…¥
        test_text = "Hello"
        print(f"\næµ‹è¯•è¾“å…¥: '{test_text}'")
        
        # Tokenize
        inputs = tokenizer(test_text, return_tensors='np', padding=True)
        input_ids = inputs['input_ids']
        print(f"âœ“ input_ids shape: {input_ids.shape}")
        
        # å°è¯•å‰å‘ä¼ æ’­
        print("\næ‰§è¡Œå‰å‘ä¼ æ’­...")
        input_ids_tensor = ms.Tensor(input_ids, dtype=ms.int32)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.set_train(False)
        
        # æ‰§è¡Œå‰å‘ä¼ æ’­
        outputs = model(input_ids_tensor)
        
        # æ£€æŸ¥è¾“å‡º
        if isinstance(outputs, tuple):
            logits = outputs[0]
        else:
            logits = outputs
        
        if hasattr(logits, 'asnumpy'):
            logits_np = logits.asnumpy()
        else:
            logits_np = np.array(logits)
        
        # ç¡®ä¿æ˜¯æµ®ç‚¹ç±»å‹
        if logits_np.dtype not in [np.float32, np.float64]:
            logits_np = logits_np.astype(np.float32)
        
        print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  - è¾“å‡ºshape: {logits_np.shape}")
        print(f"  - è¾“å‡ºdtype: {logits_np.dtype}")
        print(f"  - è¾“å‡ºç»Ÿè®¡:")
        print(f"    - min: {np.min(logits_np):.6f}")
        print(f"    - max: {np.max(logits_np):.6f}")
        print(f"    - mean: {np.mean(logits_np):.6f}")
        print(f"    - std: {np.std(logits_np):.6f}")
        
        # æ£€æŸ¥æ˜¯å¦å…¨ä¸º0
        if np.all(logits_np == 0):
            print("\nâŒ è­¦å‘Šï¼šè¾“å‡ºå…¨ä¸º0ï¼è¿™è¡¨æ˜æ¨¡å‹æƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
        if np.any(np.isnan(logits_np)) or np.any(np.isinf(logits_np)):
            print("\nâŒ è­¦å‘Šï¼šè¾“å‡ºåŒ…å«NaNæˆ–Infï¼")
            return False
        
        # è®¡ç®—softmaxå¹¶æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒ
        # å–æœ€åä¸€ä¸ªä½ç½®çš„logits
        last_logits = logits_np[0, -1, :]
        
        # ç¡®ä¿æ˜¯float32æˆ–float64
        if last_logits.dtype not in [np.float32, np.float64]:
            last_logits = last_logits.astype(np.float32)
        
        # æ‰‹åŠ¨è®¡ç®—softmax (é¿å…æ•°å€¼æº¢å‡º)
        logits_max = np.max(last_logits)
        exp_logits = np.exp(last_logits - logits_max)
        probs = exp_logits / np.sum(exp_logits)
        
        top_5_indices = np.argsort(probs)[-5:][::-1]
        top_5_probs = probs[top_5_indices]
        
        print(f"\n  - Top 5 é¢„æµ‹token:")
        for idx, prob in zip(top_5_indices, top_5_probs):
            try:
                token_text = tokenizer.decode([int(idx)], skip_special_tokens=False)
                print(f"    Token {idx}: {prob:.6f} ('{token_text}')")
            except:
                print(f"    Token {idx}: {prob:.6f}")
        
        if np.max(probs) < 0.01:
            print("\nâš ï¸ è­¦å‘Šï¼šæœ€é«˜æ¦‚ç‡å¾ˆå°ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
        
        print("\nâœ“ å¿«é€Ÿå‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"\nâŒ å¿«é€Ÿå‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥!")
        print(f"é”™è¯¯: {e}")
        import traceback
        print(traceback.format_exc())
        return False


def test_generate(model, tokenizer, prompt: str, args):
    """æµ‹è¯•generateæ¥å£"""
    print("\n" + "="*60)
    print("æµ‹è¯• generate æ¥å£")
    print("="*60)
    
    try:
        print(f"\nè¾“å…¥æç¤ºè¯(åŸå§‹): {prompt}")
        
        # åº”ç”¨chat templateï¼ˆå¯¹äºchatæ¨¡å‹éå¸¸é‡è¦ï¼‰
        print("\næ­£åœ¨åº”ç”¨chat template...")
        if hasattr(tokenizer, 'apply_chat_template') and callable(tokenizer.apply_chat_template):
            # ä½¿ç”¨æ ‡å‡†çš„chat templateæ ¼å¼
            messages = [{"role": "user", "content": prompt}]
            try:
                # å°è¯•ä½¿ç”¨apply_chat_template
                input_ids = tokenizer.apply_chat_template(
                    messages, 
                    add_generation_prompt=True,
                    return_tensors=None
                )
                if not isinstance(input_ids, list):
                    input_ids = input_ids.tolist()
                input_ids = [input_ids]  # æ·»åŠ batchç»´åº¦
                print(f"âœ“ ä½¿ç”¨tokenizer.apply_chat_templateå¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"âš  apply_chat_templateå¤±è´¥: {e}")
                print("  å›é€€åˆ°æ‰‹åŠ¨æ·»åŠ chat template...")
                # æ‰‹åŠ¨æ·»åŠ Qwen chat template
                formatted_prompt = f"<|im_start|>system\n<|im_end|>\n<|im_start|>user\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n"
                inputs = tokenizer(formatted_prompt, return_tensors='np', padding=True)
                input_ids = inputs['input_ids'].tolist()
        else:
            print("âš  tokenizeræ²¡æœ‰apply_chat_templateæ–¹æ³•ï¼Œæ‰‹åŠ¨æ·»åŠ chat template...")
            # æ‰‹åŠ¨æ·»åŠ Qwen chat template
            formatted_prompt = f"<|im_start|>system\n<|im_end|>\n<|im_start|>user\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n"
            print(f"  æ ¼å¼åŒ–åçš„prompt: {formatted_prompt[:100]}...")
            inputs = tokenizer(formatted_prompt, return_tensors='np', padding=True)
            input_ids = inputs['input_ids'].tolist()
        
        print(f"âœ“ tokenizeå®Œæˆï¼Œinput_ids shape: {np.array(input_ids).shape}")
        print(f"  input_ids: {input_ids[0][:10]}... (æ˜¾ç¤ºå‰10ä¸ª)")
        print(f"  input_idsé•¿åº¦: {len(input_ids[0])}")
        
        # æ„å»ºç”Ÿæˆé…ç½®
        print("\næ„å»ºGenerationConfig...")
        gen_config = GenerationConfig(
            max_new_tokens=args.max_new_tokens,
            do_sample=False,
            top_k=1,
            top_p=1.0,
            temperature=1.0,
            eos_token_id=int(tokenizer.eos_token_id),
            pad_token_id=int(tokenizer.pad_token_id),
            use_past=args.use_past,
            return_dict_in_generate=False,
        )
        print(f"âœ“ GenerationConfigé…ç½®å®Œæˆ")
        print(f"  - max_new_tokens: {gen_config.max_new_tokens}")
        print(f"  - use_past: {gen_config.use_past}")
        print(f"  - eos_token_id: {gen_config.eos_token_id}")
        print(f"  - pad_token_id: {gen_config.pad_token_id}")
        
        # è°ƒç”¨generate
        print("\nå¼€å§‹è°ƒç”¨model.generate()...")
        output_ids = model.generate(
            input_ids=input_ids,
            generation_config=gen_config
        )
        print("âœ“ generateè°ƒç”¨æˆåŠŸ")
        
        # è§£ç è¾“å‡º
        print("\næ­£åœ¨è§£ç è¾“å‡º...")
        if isinstance(output_ids, list):
            output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        else:
            output_text = tokenizer.decode(output_ids, skip_special_tokens=True)
        
        print("\n" + "-"*60)
        print("ç”Ÿæˆç»“æœ:")
        print("-"*60)
        print(output_text)
        print("-"*60)
        
        return True
        
    except Exception as e:
        print(f"\nâœ— generateæµ‹è¯•å¤±è´¥!")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        print("\nå®Œæ•´é”™è¯¯å †æ ˆ:")
        print(traceback.format_exc())
        return False


def test_infer(model, tokenizer, prompt: str, args):
    """æµ‹è¯•inferæ¥å£"""
    print("\n" + "="*60)
    print("æµ‹è¯• infer æ¥å£")
    print("="*60)
    
    try:
        print(f"\nè¾“å…¥æç¤ºè¯(åŸå§‹): {prompt}")
        
        # åº”ç”¨chat templateï¼ˆå¯¹äºchatæ¨¡å‹éå¸¸é‡è¦ï¼‰
        print("\næ­£åœ¨åº”ç”¨chat template...")
        if hasattr(tokenizer, 'apply_chat_template') and callable(tokenizer.apply_chat_template):
            # ä½¿ç”¨æ ‡å‡†çš„chat templateæ ¼å¼
            messages = [{"role": "user", "content": prompt}]
            try:
                # å°è¯•ä½¿ç”¨apply_chat_template
                input_ids = tokenizer.apply_chat_template(
                    messages, 
                    add_generation_prompt=True,
                    return_tensors=None
                )
                if not isinstance(input_ids, list):
                    input_ids = input_ids.tolist()
                input_ids_np = np.array([input_ids], dtype=np.int32)  # æ·»åŠ batchç»´åº¦
                print(f"âœ“ ä½¿ç”¨tokenizer.apply_chat_templateå¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"âš  apply_chat_templateå¤±è´¥: {e}")
                print("  å›é€€åˆ°æ‰‹åŠ¨æ·»åŠ chat template...")
                # æ‰‹åŠ¨æ·»åŠ Qwen chat template
                formatted_prompt = f"<|im_start|>system\n<|im_end|>\n<|im_start|>user\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n"
                inputs = tokenizer(formatted_prompt, return_tensors='np', padding=True)
                input_ids_np = inputs['input_ids']
        else:
            print("âš  tokenizeræ²¡æœ‰apply_chat_templateæ–¹æ³•ï¼Œæ‰‹åŠ¨æ·»åŠ chat template...")
            # æ‰‹åŠ¨æ·»åŠ Qwen chat template
            formatted_prompt = f"<|im_start|>system\n<|im_end|>\n<|im_start|>user\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n"
            print(f"  æ ¼å¼åŒ–åçš„prompt: {formatted_prompt[:100]}...")
            inputs = tokenizer(formatted_prompt, return_tensors='np', padding=True)
            input_ids_np = inputs['input_ids']
        
        print(f"âœ“ tokenizeå®Œæˆï¼Œinput_ids shape: {input_ids_np.shape}")
        print(f"  input_ids: {input_ids_np[0][:10].tolist()}... (æ˜¾ç¤ºå‰10ä¸ª)")
        print(f"  input_idsé•¿åº¦: {input_ids_np.shape[1]}")
        
        # è®¡ç®—æœ‰æ•ˆé•¿åº¦
        batch_size = input_ids_np.shape[0]
        pad_token_id = int(tokenizer.pad_token_id)
        valid_length_each_example = []
        for i in range(batch_size):
            # è®¡ç®—épaddingçš„é•¿åº¦
            valid_indices = np.where(input_ids_np[i] != pad_token_id)[0]
            if len(valid_indices) > 0:
                valid_len = int(np.max(valid_indices)) + 1
            else:
                valid_len = input_ids_np.shape[1]
            valid_length_each_example.append(valid_len)
        valid_length_each_example = np.array(valid_length_each_example, dtype=np.int32)
        print(f"âœ“ è®¡ç®—æœ‰æ•ˆé•¿åº¦: {valid_length_each_example}")
        
        # æ„å»ºç”Ÿæˆé…ç½®
        print("\næ„å»ºGenerationConfig...")
        gen_config = GenerationConfig(
            max_new_tokens=args.max_new_tokens,
            do_sample=False,
            top_k=1,
            top_p=1.0,
            temperature=1.0,
            eos_token_id=int(tokenizer.eos_token_id),
            pad_token_id=int(tokenizer.pad_token_id),
            use_past=args.use_past,
            return_dict_in_generate=True,
            output_scores=True,
            output_logits=True,  # å¯ç”¨ logits è¾“å‡ºï¼Œç”¨äºè¯Šæ–­
        )
        print(f"âœ“ GenerationConfigé…ç½®å®Œæˆ")
        print(f"  - use_past: {gen_config.use_past}")
        print(f"  - output_scores: {gen_config.output_scores}")
        print(f"  - output_logits: {gen_config.output_logits}")
        
        # å‡†å¤‡inferéœ€è¦çš„å‚æ•°
        is_finished = [False] * batch_size
        prefill = True
        
        # æ£€æµ‹æ¨¡å‹æ¶æ„ç±»å‹
        from mindformers.core.context import is_legacy_model
        use_legacy = is_legacy_model()
        print(f"\næ¨¡å‹æ¶æ„: {'Legacy' if use_legacy else 'MCore (Parallel Core)'}")
        
        # å‡†å¤‡ position_idsï¼ˆå¿…éœ€å‚æ•°ï¼Œä»… Legacy æ¨¡å‹éœ€è¦æ˜¾å¼ä¼ é€’ï¼‰
        print("\nå‡†å¤‡ position_ids...")
        max_len = input_ids_np.shape[1]
        position_ids = np.zeros((batch_size, max_len), dtype=np.int32)
        for idx, length in enumerate(valid_length_each_example):
            if length > 0:
                position_ids[idx, :length] = np.arange(length, dtype=np.int32)
        print(f"âœ“ position_ids shape: {position_ids.shape}")
        
        # åˆå§‹åŒ– block_mgrï¼ˆå‚è€ƒ generate() çš„é€»è¾‘ï¼‰
        print("\nåˆå§‹åŒ– block manager...")
        if not use_legacy:
            # MCore æ¨¡å‹éœ€è¦åˆå§‹åŒ–è¿™äº›ç»„ä»¶
            if hasattr(model, '_set_block_mgr'):
                try:
                    model._set_block_mgr(batch_size, model.config.seq_length)
                    print("âœ“ block_mgr åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"âš  block_mgr åˆå§‹åŒ–å¤±è´¥: {e}")
            
            if hasattr(model, '_set_kv_cache'):
                try:
                    model._set_kv_cache()
                    print("âœ“ kv_cache åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"âš  kv_cache åˆå§‹åŒ–å¤±è´¥: {e}")
            
            if hasattr(model, '_set_lower_triangle_mask'):
                try:
                    model._set_lower_triangle_mask()
                    print("âœ“ lower_triangle_mask åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"âš  lower_triangle_mask åˆå§‹åŒ–å¤±è´¥: {e}")
            
            if hasattr(model, 'set_dynamic_inputs'):
                try:
                    model.set_dynamic_inputs()
                    print("âœ“ åŠ¨æ€è¾“å…¥è®¾ç½®æˆåŠŸ")
                except Exception as e:
                    print(f"âš  åŠ¨æ€è¾“å…¥è®¾ç½®å¤±è´¥: {e}")
        elif gen_config.use_past:
            # Legacy æ¨¡å‹å¦‚æœä½¿ç”¨ use_past ä¹Ÿéœ€è¦åˆå§‹åŒ–
            if hasattr(model, '_set_block_mgr'):
                try:
                    model._set_block_mgr(batch_size, model.config.seq_length)
                    print("âœ“ block_mgr åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e:
                    print(f"âš  block_mgr åˆå§‹åŒ–å¤±è´¥: {e}")
            
            if hasattr(model, 'set_dynamic_inputs') and model.config.is_dynamic:
                try:
                    model.set_dynamic_inputs()
                    print("âœ“ åŠ¨æ€è¾“å…¥è®¾ç½®æˆåŠŸ")
                except Exception as e:
                    print(f"âš  åŠ¨æ€è¾“å…¥è®¾ç½®å¤±è´¥: {e}")
        
        # å‡†å¤‡ block_tables å’Œ slot_mapping
        print("\nå‡†å¤‡ block_tables å’Œ slot_mapping...")
        block_tables = None
        slot_mapping = None
        if hasattr(model, 'block_mgr') and model.block_mgr:
            try:
                max_input_length = input_ids_np.shape[1]
                block_tables, slot_mapping = model.block_mgr.assemble_pa_full_inputs(
                    max_input_length,
                    valid_length_each_example,
                    is_finished
                )
                print(f"âœ“ block_tables shape: {block_tables.shape if hasattr(block_tables, 'shape') else 'N/A'}")
                print(f"âœ“ slot_mapping shape: {slot_mapping.shape if hasattr(slot_mapping, 'shape') else 'N/A'}")
            except Exception as e:
                print(f"âš  å‡†å¤‡ block_tables/slot_mapping å¤±è´¥: {e}")
                import traceback
                print(traceback.format_exc())
        else:
            print(f"âš  block_mgr ä¸å­˜åœ¨æˆ–æœªåˆå§‹åŒ–")
        
        print(f"\nå¼€å§‹è°ƒç”¨ infer...")
        print(f"  - prefill: {prefill}")
        print(f"  - is_finished: {is_finished}")
        print(f"  - use_past: {gen_config.use_past}")
        
        # æ ¹æ®æ¨¡å‹æ¶æ„é€‰æ‹©è°ƒç”¨æ–¹å¼
        if use_legacy:
            # Legacy æ¨¡å‹ï¼šä½¿ç”¨æ™®é€š infer æ¥å£
            print("  - ä½¿ç”¨ Legacy infer æ¥å£")
            if block_tables is not None and slot_mapping is not None:
                infer_output, is_finished = model.infer(
                    input_ids=input_ids_np,
                    valid_length_each_example=valid_length_each_example,
                    generation_config=gen_config,
                    block_tables=block_tables,
                    slot_mapping=slot_mapping,
                    prefill=prefill,
                    is_finished=is_finished,
                    position_ids=position_ids,
                )
            else:
                infer_output, is_finished = model.infer(
                    input_ids=input_ids_np,
                    valid_length_each_example=valid_length_each_example,
                    generation_config=gen_config,
                    prefill=prefill,
                    is_finished=is_finished,
                    position_ids=position_ids,
                )
        else:
            # MCore æ¨¡å‹ï¼šä½¿ç”¨ infer_mcore æ¥å£
            # position_ids ä¼šåœ¨å†…éƒ¨è‡ªåŠ¨ç”Ÿæˆï¼ˆprepare_inputs_for_generation_mcoreï¼‰
            print("  - ä½¿ç”¨ MCore infer_mcore æ¥å£")
            if block_tables is None or slot_mapping is None:
                print("\nâœ— é”™è¯¯ï¼šblock_tables æˆ– slot_mapping æœªå‡†å¤‡å¥½")
                print("å¯èƒ½çš„åŸå› ï¼š")
                print("  1. block_mgr åˆå§‹åŒ–å¤±è´¥")
                print("  2. æ¨¡å‹é…ç½®é—®é¢˜")
                print("  3. åºåˆ—é•¿åº¦è¶…å‡ºé™åˆ¶")
                print("\nå»ºè®®ï¼š")
                print("  - æ£€æŸ¥æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­çš„ seq_length å’Œ num_blocks")
                print("  - å°è¯•å‡å°‘è¾“å…¥åºåˆ—é•¿åº¦")
                print("  - æŸ¥çœ‹ä¸Šé¢çš„åˆå§‹åŒ–æ—¥å¿—è·å–æ›´å¤šä¿¡æ¯")
                raise ValueError("MCore æ¨¡å‹éœ€è¦ block_tables å’Œ slot_mappingï¼Œä½†å‡†å¤‡å¤±è´¥")
            
            print(f"  - block_tables: {block_tables.shape if hasattr(block_tables, 'shape') else type(block_tables)}")
            print(f"  - slot_mapping: {slot_mapping.shape if hasattr(slot_mapping, 'shape') else type(slot_mapping)}")
            
            infer_output, is_finished = model.infer_mcore(
                input_ids=input_ids_np,
                valid_length_each_example=valid_length_each_example,
                generation_config=gen_config,
                block_tables=block_tables,
                slot_mapping=slot_mapping,
                prefill=prefill,
                is_finished=is_finished,
            )
        
        print("âœ“ inferè°ƒç”¨æˆåŠŸ")
        
        # è§£æè¾“å‡º
        print("\nè§£æinferè¾“å‡º...")
        print(f"  - infer_output ç±»å‹: {type(infer_output).__name__}")
        print(f"  - infer_output å€¼: {infer_output}")
        
        # å¤„ç†ä¸åŒç±»å‹çš„è¾“å‡º
        target_list = None
        probs = None
        logits = None
        
        if isinstance(infer_output, dict):
            # å­—å…¸æ ¼å¼ï¼ˆLegacy æˆ–æŸäº›é…ç½®ï¼‰
            print("  - æ£€æµ‹åˆ°å­—å…¸æ ¼å¼")
            target_list = infer_output.get("target_list")
            probs = infer_output.get("probs")
            logits = infer_output.get("logits")
        elif hasattr(infer_output, 'target_list'):
            # InferOutput å¯¹è±¡ï¼ˆå‘½åå…ƒç»„æˆ–ç±»ï¼‰
            print("  - æ£€æµ‹åˆ° InferOutput å¯¹è±¡")
            target_list = infer_output.target_list
            probs = getattr(infer_output, 'probs', None)
            logits = getattr(infer_output, 'logits', None)
            print(f"  - æå–çš„ target_list: {target_list}")
            print(f"  - æå–çš„ target_list ç±»å‹: {type(target_list)}")
        elif isinstance(infer_output, (list, tuple)):
            # ç›´æ¥è¿”å›åˆ—è¡¨æˆ–å…ƒç»„
            print("  - æ£€æµ‹åˆ°åˆ—è¡¨/å…ƒç»„æ ¼å¼")
            target_list = infer_output
        else:
            # å°è¯•ç›´æ¥ä½¿ç”¨
            print("  - ä½¿ç”¨é»˜è®¤å¤„ç†")
            target_list = infer_output
        
        print(f"\nâœ“ è§£æåçš„ target_list: {target_list}")
        print(f"  - target_list ç±»å‹: {type(target_list).__name__}")
        
        if probs is not None:
            print(f"\nâœ“ probsï¼ˆè¯è¡¨æ¦‚ç‡åˆ†å¸ƒï¼‰:")
            print(f"  - shape: {probs.shape if hasattr(probs, 'shape') else 'N/A'}")
            print(f"  - dtype: {probs.dtype if hasattr(probs, 'dtype') else 'N/A'}")
            
            # è¯¦ç»†çš„ç»Ÿè®¡è¯Šæ–­
            if hasattr(probs, 'shape') and len(probs.shape) >= 2:
                probs_np = probs[0] if len(probs.shape) == 2 else probs
                if hasattr(probs_np, 'asnumpy'):
                    probs_np = probs_np.asnumpy()
                
                # ç»Ÿè®¡ä¿¡æ¯
                print(f"\n  ğŸ“Š probs ç»Ÿè®¡è¯Šæ–­:")
                print(f"    - min: {np.min(probs_np):.10f}")
                print(f"    - max: {np.max(probs_np):.10f}")
                print(f"    - mean: {np.mean(probs_np):.10f}")
                print(f"    - sum: {np.sum(probs_np):.10f} (åº”è¯¥æ¥è¿‘ 1.0)")
                print(f"    - éé›¶å…ƒç´ æ•°: {np.count_nonzero(probs_np)}/{len(probs_np)}")
                print(f"    - >0.001 çš„å…ƒç´ æ•°: {np.sum(probs_np > 0.001)}")
                print(f"    - >0.01 çš„å…ƒç´ æ•°: {np.sum(probs_np > 0.01)}")
                
                # æ˜¾ç¤ºæ¦‚ç‡æœ€é«˜çš„å‰5ä¸ªtoken
                top_k_indices = np.argsort(probs_np)[-5:][::-1]
                top_k_probs = probs_np[top_k_indices]
                print(f"\n  - Top 5 tokens:")
                for idx, prob in zip(top_k_indices, top_k_probs):
                    try:
                        token_text = tokenizer.decode([int(idx)], skip_special_tokens=False)
                        print(f"    Token {idx}: {prob:.10f} ('{token_text}')")
                    except:
                        print(f"    Token {idx}: {prob:.10f}")
                
                # âš ï¸ å¦‚æœæ‰€æœ‰æ¦‚ç‡éƒ½æ˜¯ 0ï¼Œè¿™æ˜¯ä¸¥é‡é—®é¢˜
                if np.max(probs_np) == 0:
                    print(f"\n  âš ï¸âš ï¸âš ï¸ è­¦å‘Šï¼šæ‰€æœ‰æ¦‚ç‡éƒ½æ˜¯ 0ï¼è¿™è¡¨æ˜å¯èƒ½æœ‰é—®é¢˜ï¼š")
                    print(f"    1. æ¨¡å‹æƒé‡å¯èƒ½æ²¡æœ‰æ­£ç¡®åŠ è½½")
                    print(f"    2. æ•°å€¼è®¡ç®—å¯èƒ½ä¸‹æº¢åˆ° 0")
                    print(f"    3. éœ€è¦æ£€æŸ¥ logitsï¼ˆå¦‚æœå¯ç”¨ï¼‰")
        
        if logits is not None:
            print(f"\nâœ“ logitsï¼ˆåŸå§‹è¾“å‡ºï¼‰:")
            print(f"  - shape: {logits.shape if hasattr(logits, 'shape') else 'N/A'}")
            if hasattr(logits, 'shape'):
                logits_np = logits[0] if len(logits.shape) == 2 else logits
                if hasattr(logits_np, 'asnumpy'):
                    logits_np = logits_np.asnumpy()
                print(f"  - min: {np.min(logits_np):.6f}")
                print(f"  - max: {np.max(logits_np):.6f}")
                print(f"  - mean: {np.mean(logits_np):.6f}")
        else:
            print(f"\nâš ï¸ logits ä¸º None (output_logits=False)")
        
        # è§£ç ä¸‹ä¸€ä¸ªtoken
        if target_list is not None:
            # ç¡®ä¿ target_list æ˜¯å¯ç´¢å¼•çš„
            if isinstance(target_list, (list, tuple)):
                next_token = target_list[0]
            else:
                next_token = target_list
            
            # è½¬æ¢ä¸ºæ•´æ•°
            if hasattr(next_token, 'item'):
                # Tensor æˆ– numpy æ•°ç»„
                next_token = next_token.item()
            next_token = int(next_token)
            
            next_token_text = tokenizer.decode([next_token], skip_special_tokens=False)
            print(f"\nç”Ÿæˆçš„ä¸‹ä¸€ä¸ªtoken ID: {next_token}")
            print(f"è§£ç åçš„æ–‡æœ¬: '{next_token_text}'")
        
        print(f"\nis_finishedçŠ¶æ€: {is_finished}")
        
        return True
        
    except Exception as e:
        print(f"\nâœ— inferæµ‹è¯•å¤±è´¥!")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        import traceback
        print("\nå®Œæ•´é”™è¯¯å †æ ˆ:")
        print(traceback.format_exc())
        return False


def main():
    args = parse_args()
    
    print("\n" + "="*60)
    print("Qwen3-14B MindFormersæ¥å£æµ‹è¯•")
    print("="*60)
    print(f"æµ‹è¯•æ¨¡å¼: {args.test_mode}")
    print(f"æœ€å¤§ç”Ÿæˆtokenæ•°: {args.max_new_tokens}")
    print(f"ä½¿ç”¨å¢é‡æ¨ç†: {args.use_past}")
    print("="*60)
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = create_model_tokenizer(args)
    
    # å¿«é€Ÿå‰å‘ä¼ æ’­æµ‹è¯•
    forward_test_ok = quick_forward_test(model, tokenizer)
    if not forward_test_ok:
        print("\nâŒ å¿«é€Ÿå‰å‘ä¼ æ’­æµ‹è¯•æœªé€šè¿‡ï¼Œå»ºè®®å…ˆè§£å†³æƒé‡åŠ è½½é—®é¢˜")
        print("æ˜¯å¦ç»§ç»­è¿è¡Œå®Œæ•´æµ‹è¯•ï¼ŸæŒ‰Ctrl+Cå–æ¶ˆï¼Œæˆ–ç­‰å¾…5ç§’è‡ªåŠ¨ç»§ç»­...")
        try:
            import time
            time.sleep(5)
        except KeyboardInterrupt:
            print("\nç”¨æˆ·å–æ¶ˆæµ‹è¯•")
            return
    
    # è¿è¡Œæµ‹è¯•
    results = {}
    
    # é‡è¦ï¼šå…ˆæµ‹è¯• inferï¼Œå†æµ‹è¯• generateï¼Œé¿å…å›¾ç¼–è¯‘å†²çª
    if args.test_mode in ["infer", "both"]:
        results["infer"] = test_infer(model, tokenizer, args.prompt, args)
        # æ¸…ç† block table cache
        if hasattr(model, 'block_mgr') and model.block_mgr:
            try:
                model.block_mgr.clear_cache()
                print("\nâœ“ å·²æ¸…ç† block table cache")
            except Exception:
                pass
    
    if args.test_mode in ["generate", "both"]:
        results["generate"] = test_generate(model, tokenizer, args.prompt, args)
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    for test_name, success in results.items():
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"{test_name}: {status}")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()


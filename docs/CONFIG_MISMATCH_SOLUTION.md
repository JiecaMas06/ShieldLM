# âš ï¸ é…ç½®ä¸åŒ¹é…é—®é¢˜ - è§£å†³æ–¹æ¡ˆ

## é—®é¢˜æè¿°

è¿è¡Œæ—¶å‡ºçŽ°ä»¥ä¸‹é”™è¯¯ï¼š

```
RuntimeError: For 'load_param_into_net', embedding.word_embeddings.weight in the argument 'net' 
should have the same shape as embedding.word_embeddings.weight in the argument 'parameter_dict'. 
But got its shape (151936, 4096) in the argument 'net' and shape (151936, 5120) in the argument 'parameter_dict'.
```

## é—®é¢˜åŽŸå› 

**é…ç½®æ–‡ä»¶ä¸Žæƒé‡æ–‡ä»¶ä¸åŒ¹é…**ï¼

| é¡¹ç›® | é…ç½®æ–‡ä»¶ (YAML) | æƒé‡æ–‡ä»¶ | è¯´æ˜Ž |
|------|----------------|---------|------|
| `hidden_size` | 4096 | 5120 | âŒ ä¸åŒ¹é… |
| `vocab_size` | 151936 | 151936 | âœ… åŒ¹é… |

æ‚¨çš„é…ç½®æ–‡ä»¶è®¾ç½®äº† `hidden_size=4096`ï¼Œä½†æƒé‡æ–‡ä»¶å®žé™…æ˜¯ `hidden_size=5120`ï¼ˆQwen3-14B çš„æ­£ç¡®å€¼ï¼‰ã€‚

## å¿«é€Ÿè§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ä½¿ç”¨æƒé‡ç›®å½•çš„ config.jsonï¼ˆæŽ¨èï¼‰â­

**æœ€ç®€å•çš„æ–¹æ³•**ï¼šç¡®ä¿æƒé‡ç›®å½•åŒ…å« `config.json` æ–‡ä»¶ã€‚

```bash
# æ£€æŸ¥æƒé‡ç›®å½•
ls /path/to/Qwen3-14B/

# åº”è¯¥çœ‹åˆ°ï¼š
# config.json                      â† å¿…é¡»æœ‰è¿™ä¸ªæ–‡ä»¶ï¼
# tokenizer.json
# tokenizer_config.json
# model-00001-of-00008.safetensors
# ...
```

å¦‚æžœæœ‰ `config.json`ï¼Œè„šæœ¬ä¼š**è‡ªåŠ¨**ä»Žè¯¥æ–‡ä»¶åŠ è½½æ­£ç¡®çš„é…ç½®ï¼Œæ— éœ€æ‰‹åŠ¨ä¿®æ”¹ï¼

```bash
# è¿è¡Œæ—¶è„šæœ¬ä¼šè‡ªåŠ¨ä½¿ç”¨ config.json
python ShieldLM/test_qwen3_direct.py \
    --config your_config.yaml \
    --model_dir /path/to/Qwen3-14B \
    --use_training_conversion
```

**é¢„æœŸè¾“å‡º**ï¼š
```
============================================================
æ£€æŸ¥æƒé‡ç›®å½•çš„ config.json
============================================================
  âœ“ æ‰¾åˆ° config.json
  å…³é”®é…ç½®é¡¹:
    - hidden_size: 5120
    - num_hidden_layers: 48
    - num_attention_heads: 40
    - num_key_value_heads: 8
    - vocab_size: 151936
    - intermediate_size: 13824

============================================================
åˆ›å»ºQwen3Config
============================================================
å°è¯•ä»Žæƒé‡ç›®å½•åŠ è½½ config.json: /path/to/Qwen3-14B
âœ“ ä»Žæƒé‡ç›®å½•çš„ config.json åŠ è½½é…ç½®æˆåŠŸ
```

### æ–¹æ¡ˆ 2: æ‰‹åŠ¨ä¿®æ”¹ YAML é…ç½®æ–‡ä»¶

å¦‚æžœæƒé‡ç›®å½•æ²¡æœ‰ `config.json`ï¼Œéœ€è¦æ‰‹åŠ¨ä¿®æ”¹ YAML é…ç½®æ–‡ä»¶ã€‚

#### Qwen3-14B çš„æ­£ç¡®é…ç½®ï¼š

```yaml
model:
  model_config:
    type: Qwen3Config
    vocab_size: 151936
    hidden_size: 5120          # â­ æ”¹ä¸º 5120
    num_hidden_layers: 48       # â­ æ”¹ä¸º 48
    num_attention_heads: 40     # â­ æ”¹ä¸º 40
    num_key_value_heads: 8      # â­ æ”¹ä¸º 8
    intermediate_size: 13824    # â­ æ”¹ä¸º 13824
    max_position_embeddings: 32768
    rms_norm_eps: 1.0e-6
    rope_theta: 1000000.0
    attention_bias: true
```

## å¸¸è§ Qwen3 æ¨¡åž‹é…ç½®

| æ¨¡åž‹ | hidden_size | num_layers | num_heads | num_kv_heads | intermediate_size |
|------|-------------|------------|-----------|--------------|-------------------|
| Qwen3-0.5B | 896 | 24 | 14 | 2 | 4864 |
| Qwen3-1.8B | 2048 | 28 | 16 | 4 | 11008 |
| Qwen3-4B | 2560 | 40 | 20 | 4 | 13824 |
| Qwen3-7B | 3584 | 28 | 28 | 4 | 18944 |
| **Qwen3-14B** | **5120** | **48** | **40** | **8** | **13824** |
| Qwen3-32B | 5120 | 64 | 40 | 8 | 27648 |

## è„šæœ¬çš„è‡ªåŠ¨ä¿®å¤åŠŸèƒ½

æ›´æ–°åŽçš„ `test_qwen3_direct.py` åŒ…å«ä»¥ä¸‹è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤åŠŸèƒ½ï¼š

### 1. è‡ªåŠ¨æ£€æµ‹ config.json

```python
# è„šæœ¬ä¼šè‡ªåŠ¨æ£€æŸ¥æƒé‡ç›®å½•
if model_dir and os.path.exists(os.path.join(model_dir, "config.json")):
    # ä¼˜å…ˆä»Ž config.json åŠ è½½
    qwen3_config = Qwen3Config.from_pretrained(model_dir)
```

### 2. æƒé‡éªŒè¯

åœ¨åŠ è½½æƒé‡å‰ï¼Œè„šæœ¬ä¼šéªŒè¯é…ç½®æ˜¯å¦åŒ¹é…ï¼š

```
============================================================
éªŒè¯æƒé‡ä¸Žé…ç½®çš„åŒ¹é…æ€§
============================================================

ä»Žæƒé‡æ£€æµ‹åˆ°çš„é…ç½®:
  - vocab_size: 151936
  - hidden_size: 5120

å½“å‰æ¨¡åž‹é…ç½®:
  - vocab_size: 151936
  - hidden_size: 4096

âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸
æ£€æµ‹åˆ°é…ç½®ä¸åŒ¹é…é—®é¢˜:
  âŒ hidden_size ä¸åŒ¹é…ï¼šæƒé‡=5120, é…ç½®=4096
âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸âš ï¸
```

### 3. è‡ªåŠ¨ä¿®å¤

å¦‚æžœæ£€æµ‹åˆ°ä¸åŒ¹é…ï¼Œè„šæœ¬ä¼šï¼š
1. æ˜¾ç¤ºè¯¦ç»†çš„ä¿®å¤å»ºè®®
2. è‡ªåŠ¨ä½¿ç”¨æ£€æµ‹åˆ°çš„é…ç½®é‡æ–°åˆ›å»ºæ¨¡åž‹
3. ç»§ç»­åŠ è½½æƒé‡

```
å°è¯•ä½¿ç”¨æ£€æµ‹åˆ°çš„é…ç½®é‡æ–°åˆ›å»ºæ¨¡åž‹...
âœ“ é…ç½®å·²æ›´æ–°:
  - vocab_size: 151936
  - hidden_size: 5120

é‡æ–°åˆ›å»ºæ¨¡åž‹å®žä¾‹...
âœ“ æ¨¡åž‹å®žä¾‹é‡æ–°åˆ›å»ºæˆåŠŸ
```

## å®Œæ•´ä½¿ç”¨æµç¨‹

### æ­¥éª¤ 1: å‡†å¤‡æƒé‡æ–‡ä»¶

ç¡®ä¿æƒé‡ç›®å½•åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

```
Qwen3-14B/
â”œâ”€â”€ config.json                      â† â­ æœ€é‡è¦ï¼
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ model-00001-of-00008.safetensors
â”œâ”€â”€ model-00002-of-00008.safetensors
â”œâ”€â”€ ...
â””â”€â”€ model.safetensors.index.json
```

**å¦‚ä½•èŽ·å– config.json**ï¼š
- å¦‚æžœä»Ž HuggingFace ä¸‹è½½ï¼Œåº”è¯¥ä¼šè‡ªåŠ¨åŒ…å«
- å¦‚æžœç¼ºå¤±ï¼Œå¯ä»¥ä»Ž HuggingFace æ¨¡åž‹é¡µé¢ä¸‹è½½
- Qwen3-14B: https://huggingface.co/Qwen/Qwen2.5-14B/blob/main/config.json

### æ­¥éª¤ 2: è¿è¡Œè„šæœ¬

```bash
python ShieldLM/test_qwen3_direct.py \
    --config your_config.yaml \
    --model_dir /path/to/Qwen3-14B \
    --tokenizer_path /path/to/Qwen3-14B \
    --use_training_conversion \
    --test_mode both
```

### æ­¥éª¤ 3: éªŒè¯è¾“å‡º

æˆåŠŸæ—¶åº”è¯¥çœ‹åˆ°ï¼š

```
âœ“ æ‰¾åˆ° config.json
âœ“ ä»Žæƒé‡ç›®å½•çš„ config.json åŠ è½½é…ç½®æˆåŠŸ
âœ“ æƒé‡ä¸Žé…ç½®åŒ¹é…
âœ“ æ‰€æœ‰æƒé‡åŠ è½½æˆåŠŸ
```

## å¦‚æžœä»ç„¶å¤±è´¥

### æ£€æŸ¥æ¸…å•

1. **ç¡®è®¤æ¨¡åž‹ç±»åž‹**
   ```bash
   # æŸ¥çœ‹æƒé‡æ–‡ä»¶å¤§å°
   du -sh /path/to/Qwen3-14B/*.safetensors
   
   # Qwen3-14B (BF16) åº”è¯¥çº¦ 28GB
   # å¦‚æžœå·®å¼‚å¾ˆå¤§ï¼Œå¯èƒ½ä¸‹è½½çš„æ˜¯å…¶ä»–æ¨¡åž‹
   ```

2. **éªŒè¯ config.json å†…å®¹**
   ```bash
   cat /path/to/Qwen3-14B/config.json | grep hidden_size
   # åº”è¯¥æ˜¾ç¤º: "hidden_size": 5120
   ```

3. **æ£€æŸ¥ YAML é…ç½®**
   ```bash
   cat your_config.yaml | grep hidden_size
   # å¦‚æžœæ²¡æœ‰ config.jsonï¼Œç¡®ä¿ YAML ä¸­æ˜¯ 5120
   ```

### æ‰‹åŠ¨åˆ›å»º config.json

å¦‚æžœæƒé‡ç›®å½•çœŸçš„æ²¡æœ‰ `config.json`ï¼Œå¯ä»¥æ‰‹åŠ¨åˆ›å»ºï¼š

```bash
cat > /path/to/Qwen3-14B/config.json << 'EOF'
{
  "architectures": ["Qwen2ForCausalLM"],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 48,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936,
  "attention_bias": true
}
EOF
```

## è¯Šæ–­å‘½ä»¤

### å¿«é€Ÿæ£€æŸ¥é…ç½®

```bash
# æ£€æŸ¥æƒé‡ç›®å½•
ls -lh /path/to/Qwen3-14B/

# æŸ¥çœ‹ config.jsonï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
python -c "import json; print(json.load(open('/path/to/Qwen3-14B/config.json'))['hidden_size'])"

# åº”è¯¥è¾“å‡º: 5120
```

### éªŒè¯æƒé‡æ–‡ä»¶

```python
# å¿«é€ŸéªŒè¯è„šæœ¬
import os
from safetensors import safe_open

model_dir = "/path/to/Qwen3-14B"
safetensors_files = [f for f in os.listdir(model_dir) if f.endswith('.safetensors')]

if safetensors_files:
    first_file = os.path.join(model_dir, safetensors_files[0])
    with safe_open(first_file, framework="np") as f:
        # æŸ¥æ‰¾ embedding å±‚
        for key in f.keys():
            if 'embed_tokens' in key:
                tensor = f.get_tensor(key)
                print(f"Key: {key}")
                print(f"Shape: {tensor.shape}")
                print(f"Hidden size: {tensor.shape[1]}")
                break
```

## æ€»ç»“

**æœ€ä½³å®žè·µ**ï¼š
1. âœ… ç¡®ä¿æƒé‡ç›®å½•æœ‰ `config.json`
2. âœ… ä½¿ç”¨ `--use_training_conversion` å‚æ•°
3. âœ… è®©è„šæœ¬è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤é…ç½®ä¸åŒ¹é…

**ä¸æŽ¨è**ï¼š
1. âŒ æ‰‹åŠ¨çŒœæµ‹é…ç½®å‚æ•°
2. âŒ ä¸æ£€æŸ¥å°±è¿è¡Œ
3. âŒ å¿½ç•¥é…ç½®ä¸åŒ¹é…çš„è­¦å‘Š

æŒ‰ç…§ä»¥ä¸Šæ­¥éª¤ï¼Œé…ç½®ä¸åŒ¹é…é—®é¢˜åº”è¯¥å¯ä»¥å®Œå…¨è§£å†³ï¼ðŸŽ‰


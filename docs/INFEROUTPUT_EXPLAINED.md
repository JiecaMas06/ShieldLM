# InferOutput å¯¹è±¡è¯¦è§£

## ğŸ“¦ InferOutput çš„ç»“æ„

æ ¹æ®ä½ çš„è§‚å¯Ÿï¼Œ`infer_mcore()` è¿”å›çš„ `InferOutput` å¯¹è±¡åŒ…å«ä¸‰ä¸ªå­—æ®µï¼š

```python
InferOutput(
    target_list=[token_id],           # ç”Ÿæˆçš„ token ID åˆ—è¡¨
    probs=array([[...]], dtype=...),  # è¯è¡¨æ¦‚ç‡åˆ†å¸ƒ
    logits=None or array([...])       # åŸå§‹ logitsï¼ˆå¯é€‰ï¼‰
)
```

### 1. target_list (ç”Ÿæˆçš„ token)

**ç±»å‹**: `list[int]`

**å«ä¹‰**: æ¨¡å‹é¢„æµ‹çš„ä¸‹ä¸€ä¸ªï¼ˆæˆ–å¤šä¸ªï¼‰token çš„ ID

**ç¤ºä¾‹**:
```python
target_list = [0]        # é¢„æµ‹çš„ token ID æ˜¯ 0
target_list = [151645]   # é¢„æµ‹çš„ token ID æ˜¯ 151645 (å¯èƒ½æ˜¯ <|im_end|>)
```

**ç”¨é€”**:
```python
next_token_id = target_list[0]
next_token_text = tokenizer.decode([next_token_id])
```

### 2. probs (æ¦‚ç‡åˆ†å¸ƒ)

**ç±»å‹**: `numpy.ndarray`

**å½¢çŠ¶**: `(batch_size, vocab_size)` æˆ– `(vocab_size,)`

**å«ä¹‰**: **æ•´ä¸ªè¯è¡¨**çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ¯ä¸ªä½ç½®å¯¹åº”ä¸€ä¸ª token çš„æ¦‚ç‡

**ç¤ºä¾‹**:
```python
probs = array([[0.001, 0.002, ..., 0.003]], dtype=float32)
# å½¢çŠ¶: (1, 151936) - Qwen3 çš„è¯è¡¨å¤§å°æ˜¯ 151936

# probs[0][0] = token 0 çš„æ¦‚ç‡
# probs[0][1] = token 1 çš„æ¦‚ç‡
# ...
# probs[0][151935] = token 151935 çš„æ¦‚ç‡
```

**ç‰¹ç‚¹**:
- âœ… æ¦‚ç‡ä¹‹å’Œä¸º 1.0ï¼ˆç»è¿‡ softmaxï¼‰
- âœ… åŒ…å«æ‰€æœ‰å¯èƒ½ token çš„æ¦‚ç‡
- âœ… å¯ä»¥ç”¨äºåˆ†ææ¨¡å‹çš„ä¸ç¡®å®šæ€§
- âœ… å¯ä»¥ç”¨äºé‡‡æ ·ç­–ç•¥ï¼ˆtop-k, top-p ç­‰ï¼‰

**ç”¨é€”**:
```python
import numpy as np

# æ‰¾å‡ºæ¦‚ç‡æœ€é«˜çš„ token
top_token = np.argmax(probs[0])

# æ‰¾å‡ºæ¦‚ç‡æœ€é«˜çš„å‰ 5 ä¸ª tokens
top_5_indices = np.argsort(probs[0])[-5:][::-1]
top_5_probs = probs[0][top_5_indices]

# è®¡ç®—ç†µï¼ˆä¸ç¡®å®šæ€§ï¼‰
entropy = -np.sum(probs[0] * np.log(probs[0] + 1e-10))

# é‡‡æ ·ï¼ˆæ ¹æ®æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ï¼‰
sampled_token = np.random.choice(len(probs[0]), p=probs[0])
```

### 3. logits (åŸå§‹è¾“å‡º)

**ç±»å‹**: `numpy.ndarray` æˆ– `None`

**å½¢çŠ¶**: `(batch_size, vocab_size)` æˆ– `(vocab_size,)`

**å«ä¹‰**: æ¨¡å‹æœ€åä¸€å±‚çš„åŸå§‹è¾“å‡ºï¼ˆæœªç» softmaxï¼‰

**ç¤ºä¾‹**:
```python
logits = array([[2.5, -1.3, ..., 0.8]], dtype=float32)
# æˆ–
logits = None  # å¦‚æœ output_logits=False
```

**å…³ç³»**:
```python
# probs æ˜¯ logits ç»è¿‡ softmax å¾—åˆ°çš„
probs = softmax(logits)

# å¯ä»¥åå‘è®¡ç®— logitsï¼ˆå¦‚æœéœ€è¦ï¼‰
logits_approx = np.log(probs + 1e-10)
```

## ğŸ” ä¸ run_mindformers_probability.py çš„å¯¹åº”å…³ç³»

åœ¨ `run_mindformers_probability.py` ä¸­ï¼š

```python
def _collect_infer_scores(model, tokenized_batch, infer_config, steps_needed, pad_token_id):
    score_history: List[np.ndarray] = []
    
    for _ in range(steps_needed):
        infer_output, is_finished = model.infer(...)
        
        # è·å– probs
        probs_tensor = infer_output["probs"]  # æˆ– infer_output.probs
        score_history.append(_ms_to_np(probs_tensor))
        
        # è·å–ä¸‹ä¸€ä¸ª token
        target_list = infer_output["target_list"]  # æˆ– infer_output.target_list
        for idx, token in enumerate(target_list):
            sequences[idx].append(int(token))
    
    return score_history
```

### get_probs å‡½æ•°çš„å®ç°

```python
def get_probs(scores: List[np.ndarray], idx: int, lang: str, model_base: str):
    # scores æ˜¯å¤šæ­¥çš„ probs åˆ—è¡¨
    # scores[0] = ç¬¬ä¸€æ­¥çš„ probs (1, vocab_size)
    # scores[1] = ç¬¬äºŒæ­¥çš„ probs (1, vocab_size)
    # ...
    
    token_place, safe_token, unsafe_token, controversial_token = _select_token_info(lang, model_base)
    
    # é€‰æ‹©ç‰¹å®šä½ç½®çš„ probs
    if token_place >= len(scores):
        token_place = len(scores) - 1
    
    score_np = _ms_to_np(scores[token_place])[idx].astype(np.float32)
    
    # ä»å®Œæ•´çš„è¯è¡¨æ¦‚ç‡ä¸­æå–ç‰¹å®š tokens
    valid_scores = np.array([
        score_np[safe_token],           # safe token çš„æ¦‚ç‡
        score_np[unsafe_token],         # unsafe token çš„æ¦‚ç‡
        score_np[controversial_token]   # controversial token çš„æ¦‚ç‡
    ], dtype=np.float32)
    
    # å½’ä¸€åŒ–ä¸ºä¸‰åˆ†ç±»æ¦‚ç‡
    max_valid = np.max(valid_scores)
    exp_scores = np.exp(valid_scores - max_valid)
    probs = exp_scores / np.sum(exp_scores)
    
    return {
        'safe': float(probs[0]),
        'unsafe': float(probs[1]),
        'controversial': float(probs[2])
    }
```

## ğŸ“Š å®é™…ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºæœ¬ä½¿ç”¨

```python
# è°ƒç”¨ infer_mcore
infer_output, is_finished = model.infer_mcore(...)

# æå–å­—æ®µ
target_list = infer_output.target_list  # [151645]
probs = infer_output.probs              # (1, 151936)
logits = infer_output.logits            # None æˆ– (1, 151936)

# ä½¿ç”¨ token
next_token = target_list[0]
token_text = tokenizer.decode([next_token])

# åˆ†ææ¦‚ç‡
token_prob = probs[0][next_token]
print(f"Token {next_token} çš„æ¦‚ç‡: {token_prob:.6f}")
```

### ç¤ºä¾‹ 2: Top-K åˆ†æ

```python
import numpy as np

probs_np = infer_output.probs[0]  # (vocab_size,)

# æ‰¾å‡ºæ¦‚ç‡æœ€é«˜çš„ 10 ä¸ª tokens
top_k = 10
top_indices = np.argsort(probs_np)[-top_k:][::-1]
top_probs = probs_np[top_indices]

print("Top-10 tokens:")
for idx, prob in zip(top_indices, top_probs):
    token_text = tokenizer.decode([int(idx)])
    print(f"  {idx:6d} ({prob:8.6f}): '{token_text}'")
```

### ç¤ºä¾‹ 3: ç‰¹å®š Token çš„æ¦‚ç‡

```python
# æŸ¥è¯¢ç‰¹å®š token çš„æ¦‚ç‡ï¼ˆå¦‚ ShieldLM çš„åˆ†ç±» tokensï¼‰
safe_token_id = 41479      # "safe" çš„ token ID
unsafe_token_id = 86009    # "unsafe" çš„ token ID

probs_np = infer_output.probs[0]

safe_prob = probs_np[safe_token_id]
unsafe_prob = probs_np[unsafe_token_id]

print(f"Safe æ¦‚ç‡: {safe_prob:.6f}")
print(f"Unsafe æ¦‚ç‡: {unsafe_prob:.6f}")
```

### ç¤ºä¾‹ 4: ç†µå’Œä¸ç¡®å®šæ€§

```python
import numpy as np

probs_np = infer_output.probs[0]

# è®¡ç®—ç†µï¼ˆä¿¡æ¯ç†µï¼‰
entropy = -np.sum(probs_np * np.log(probs_np + 1e-10))
print(f"ç†µ: {entropy:.4f}")

# å½’ä¸€åŒ–ç†µï¼ˆ0-1 ä¹‹é—´ï¼‰
max_entropy = np.log(len(probs_np))
normalized_entropy = entropy / max_entropy
print(f"å½’ä¸€åŒ–ç†µ: {normalized_entropy:.4f}")

# ç†µè¶Šé«˜ï¼Œæ¨¡å‹è¶Šä¸ç¡®å®š
if normalized_entropy > 0.5:
    print("æ¨¡å‹ä¸ç¡®å®šæ€§è¾ƒé«˜")
else:
    print("æ¨¡å‹æ¯”è¾ƒç¡®å®š")
```

## ğŸ¯ å…³é”®è¦ç‚¹

### 1. è¯è¡¨å¤§å°

Qwen3-14B çš„è¯è¡¨å¤§å°æ˜¯ **151,936**ï¼Œæ‰€ä»¥ `probs` çš„å½¢çŠ¶æ˜¯ `(1, 151936)` æˆ– `(151936,)`ã€‚

### 2. æ¦‚ç‡åˆ†å¸ƒçš„ç‰¹ç‚¹

- **æ‰€æœ‰æ¦‚ç‡ä¹‹å’Œä¸º 1.0**
- å¤§å¤šæ•° token çš„æ¦‚ç‡éå¸¸å°ï¼ˆæ¥è¿‘ 0ï¼‰
- åªæœ‰å°‘æ•° token æœ‰è¾ƒé«˜çš„æ¦‚ç‡
- é¢„æµ‹çš„ token é€šå¸¸æ˜¯æ¦‚ç‡æœ€é«˜çš„é‚£ä¸ªï¼ˆgreedy æ¨¡å¼ï¼‰

### 3. ä¸ generate çš„å…³ç³»

`generate()` å†…éƒ¨ä¼šå¤šæ¬¡è°ƒç”¨ `infer()` æˆ– `infer_mcore()`ï¼š

```
generate() å¾ªç¯:
  â”œâ”€ ç¬¬1æ¬¡: infer_mcore() â†’ probs[0], target[0]
  â”œâ”€ ç¬¬2æ¬¡: infer_mcore() â†’ probs[1], target[1]
  â”œâ”€ ç¬¬3æ¬¡: infer_mcore() â†’ probs[2], target[2]
  â””â”€ ...
```

æ¯æ¬¡ infer è¿”å›ï¼š
- å½“å‰ä½ç½®çš„å®Œæ•´è¯è¡¨æ¦‚ç‡åˆ†å¸ƒ
- æ ¹æ®ç­–ç•¥é€‰æ‹©çš„ token ID

### 4. é‡‡æ ·ç­–ç•¥çš„åº”ç”¨

ä¸åŒçš„é‡‡æ ·ç­–ç•¥ä½¿ç”¨ `probs`ï¼š

| ç­–ç•¥ | ä½¿ç”¨æ–¹å¼ |
|------|---------|
| Greedy | `argmax(probs)` |
| Top-K | åªä¿ç•™æ¦‚ç‡æœ€é«˜çš„ K ä¸ªï¼Œé‡æ–°å½’ä¸€åŒ– |
| Top-P | ä¿ç•™ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° P çš„ tokens |
| Temperature | è°ƒæ•´ logits: `logits / temperature` |

## ğŸ“ è°ƒè¯•å»ºè®®

åœ¨ä½ çš„æµ‹è¯•ä»£ç ä¸­ï¼Œç°åœ¨ä¼šæ˜¾ç¤ºï¼š

```
è§£æinferè¾“å‡º...
  - infer_output ç±»å‹: InferOutput
  - æ£€æµ‹åˆ° InferOutput å¯¹è±¡
  - æå–çš„ target_list: [151645]
  
âœ“ è§£æåçš„ target_list: [151645]
  - target_list ç±»å‹: list

âœ“ probsï¼ˆè¯è¡¨æ¦‚ç‡åˆ†å¸ƒï¼‰:
  - shape: (1, 151936)
  - dtype: float32
  - Top 5 tokens:
    Token 151645: 0.850000 ('<|im_end|>')
    Token 108386: 0.080000 ('ä½ ')
    Token 151643: 0.030000 ('<|endoftext|>')
    ...
```

è¿™æ ·ä½ å°±èƒ½æ¸…æ¥šåœ°çœ‹åˆ°æ¨¡å‹çš„é¢„æµ‹åˆ†å¸ƒäº†ï¼

## ğŸ”— ç›¸å…³æ–‡æ¡£

- `run_mindformers_probability.py` - æ¦‚ç‡æå–çš„å®Œæ•´å®ç°
- `PROBLEM_5_FIX.md` - InferOutput ç±»å‹å¤„ç†
- `test_qwen3_mindformers.py` - æµ‹è¯•ä»£ç 

---

å¸Œæœ›è¿™ä¸ªæ–‡æ¡£å¸®åŠ©ä½ ç†è§£ InferOutput çš„ç»“æ„å’Œç”¨é€”ï¼ğŸ‰

